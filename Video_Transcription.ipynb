{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "POlrjEE900gz",
        "outputId": "8285f461-70cb-43a3-e972-67c33363b082"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yt-dlp in /usr/local/lib/python3.12/dist-packages (2025.11.12)\n",
            "Requirement already satisfied: openai-whisper in /usr/local/lib/python3.12/dist-packages (20250625)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.12/dist-packages (5.50.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.8.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.12.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.5.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (4.11.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.2.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.118.3)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.12/dist-packages (from gradio) (1.0.0)\n",
            "Requirement already satisfied: gradio-client==1.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (1.14.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.36.0)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.0.3)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (3.11.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<=2.12.3,>=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.12.3)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (6.0.3)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.14.6)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.1.7)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.48.0)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.20.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from gradio) (0.38.0)\n",
            "Requirement already satisfied: websockets<16.0,>=13.0 in /usr/local/lib/python3.12/dist-packages (from gradio-client==1.14.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<=2.12.3,>=2.0->gradio) (0.4.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (2.5.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://cli.github.com/packages stable InRelease\n",
            "Get:4 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 384 kB in 1s (263 kB/s)\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "The following additional packages will be installed:\n",
            "  javascript-common libc-ares2 libjs-highlight.js libnode72 nodejs-doc\n",
            "Suggested packages:\n",
            "  apache2 | lighttpd | httpd npm\n",
            "The following NEW packages will be installed:\n",
            "  javascript-common libc-ares2 libjs-highlight.js libnode72 nodejs nodejs-doc\n",
            "0 upgraded, 6 newly installed, 0 to remove and 58 not upgraded.\n",
            "Need to get 13.7 MB of archives.\n",
            "After this operation, 54.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 javascript-common all 11+nmu1 [5,936 B]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libjs-highlight.js all 9.18.5+dfsg1-1 [367 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.3 [45.1 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libnode72 amd64 12.22.9~dfsg-1ubuntu3.6 [10.8 MB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 nodejs-doc all 12.22.9~dfsg-1ubuntu3.6 [2,411 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 nodejs amd64 12.22.9~dfsg-1ubuntu3.6 [122 kB]\n",
            "Fetched 13.7 MB in 3s (4,400 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package javascript-common.\n",
            "(Reading database ... 121713 files and directories currently installed.)\n",
            "Preparing to unpack .../0-javascript-common_11+nmu1_all.deb ...\n",
            "Unpacking javascript-common (11+nmu1) ...\n",
            "Selecting previously unselected package libjs-highlight.js.\n",
            "Preparing to unpack .../1-libjs-highlight.js_9.18.5+dfsg1-1_all.deb ...\n",
            "Unpacking libjs-highlight.js (9.18.5+dfsg1-1) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../2-libc-ares2_1.18.1-1ubuntu0.22.04.3_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Selecting previously unselected package libnode72:amd64.\n",
            "Preparing to unpack .../3-libnode72_12.22.9~dfsg-1ubuntu3.6_amd64.deb ...\n",
            "Unpacking libnode72:amd64 (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Selecting previously unselected package nodejs-doc.\n",
            "Preparing to unpack .../4-nodejs-doc_12.22.9~dfsg-1ubuntu3.6_all.deb ...\n",
            "Unpacking nodejs-doc (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Selecting previously unselected package nodejs.\n",
            "Preparing to unpack .../5-nodejs_12.22.9~dfsg-1ubuntu3.6_amd64.deb ...\n",
            "Unpacking nodejs (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Setting up javascript-common (11+nmu1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.3) ...\n",
            "Setting up libnode72:amd64 (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Setting up libjs-highlight.js (9.18.5+dfsg1-1) ...\n",
            "Setting up nodejs (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "update-alternatives: using /usr/bin/nodejs to provide /usr/bin/js (js) in auto mode\n",
            "Setting up nodejs-doc (12.22.9~dfsg-1ubuntu3.6) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "Loading Whisper model 'base' on cuda...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 139M/139M [00:00<00:00, 189MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "â¬‡ï¸ Downloading audio from https://www.youtube.com/watch?v=jNQXAC9IVRw using cookies from: /content/cookies.txt\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=jNQXAC9IVRw\n",
            "[youtube] jNQXAC9IVRw: Downloading webpage\n",
            "[youtube] jNQXAC9IVRw: Downloading tv downgraded player API JSON\n",
            "[youtube] jNQXAC9IVRw: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] jNQXAC9IVRw: n challenge solving failed: Some formats may be missing. Ensure you have a supported JavaScript runtime and challenge solver script distribution installed. Review any warnings presented before this message. For more details, refer to  https://github.com/yt-dlp/yt-dlp/wiki/EJS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] jNQXAC9IVRw: Downloading m3u8 information\n",
            "[info] jNQXAC9IVRw: Downloading 1 format(s): 92\n",
            "[download] Sleeping 6.00 seconds as required by the site...\n",
            "[hlsnative] Downloading m3u8 manifest\n",
            "[hlsnative] Total fragments: 4\n",
            "[download] Destination: temp_audio\n",
            "[download] 100% of  621.28KiB in 00:00:00 at 2.02MiB/s                 \n",
            "[FixupM3u8] Fixing MPEG-TS in MP4 container of \"temp_audio\"\n",
            "[ExtractAudio] Destination: temp_audio.mp3\n",
            "Deleting original file temp_audio (pass -k to keep)\n",
            " Transcribing audio (with word timestamps)...\n",
            "â±ï¸ Aligning sentences to timestamps...\n",
            "ðŸ§  Performing semantic chunking...\n",
            "[\n",
            "    {\n",
            "        \"chunk_id\": 1,\n",
            "        \"chunk_length\": 12.48,\n",
            "        \"text\": \"Alright so here we are one of the elephants. The cool thing about these guys is that they have really, really, really long hunts. And that's cool.\",\n",
            "        \"start_time\": 1.1,\n",
            "        \"end_time\": 13.58\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 2,\n",
            "        \"chunk_length\": 4.5,\n",
            "        \"text\": \"And that's pretty much all there is to say.\",\n",
            "        \"start_time\": 13.58,\n",
            "        \"end_time\": 18.08\n",
            "    }\n",
            "]\n"
          ]
        }
      ],
      "source": [
        "!pip install yt-dlp openai-whisper nltk torch gradio pydub\n",
        "\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install -y ffmpeg nodejs\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import time\n",
        "import torch\n",
        "import nltk\n",
        "import whisper\n",
        "import yt_dlp\n",
        "import gradio as gr\n",
        "from datetime import timedelta\n",
        "\n",
        "# --- Configuration and Initialization ---\n",
        "# Ensure NLTK data is ready\n",
        "try:\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "class RobustVideoChunker:\n",
        "    def __init__(self, model_size=\"base\"):\n",
        "        \"\"\"\n",
        "        Initialize the Whisper model.\n",
        "        Use 'large-v3' or 'medium.en' for high-quality production results.\n",
        "        'base' is used here for faster testing/demo purposes.\n",
        "        \"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        print(f\"Loading Whisper model '{model_size}' on {self.device}...\")\n",
        "        self.model = whisper.load_model(model_size, device=self.device)\n",
        "        print(\"Model loaded.\")\n",
        "\n",
        "    def download_audio(self, youtube_url, cookies_path):\n",
        "        \"\"\"\n",
        "        Downloads audio using the cookies method to bypass bot protection.\n",
        "\n",
        "        Args:\n",
        "            youtube_url (str): The YouTube link.\n",
        "            cookies_path (str): Path to the Netscape-formatted cookies file.\n",
        "        \"\"\"\n",
        "        print(f\"â¬‡ï¸ Downloading audio from {youtube_url} using cookies from: {cookies_path}\")\n",
        "\n",
        "        ydl_opts = {\n",
        "            'format': 'bestaudio/best',\n",
        "            'postprocessors': [{'key': 'FFmpegExtractAudio', 'preferredcodec': 'mp3', 'preferredquality': '192'}],\n",
        "            'outtmpl': 'temp_audio',\n",
        "            'quiet': False,\n",
        "            'nocheckcertificate': True,\n",
        "            'cookiefile': cookies_path,\n",
        "        }\n",
        "\n",
        "        output_path = \"audio.mp3\"\n",
        "\n",
        "        try:\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([youtube_url])\n",
        "\n",
        "            # Rename output\n",
        "            if os.path.exists(\"temp_audio.mp3\"):\n",
        "                if os.path.exists(output_path): os.remove(output_path)\n",
        "                os.rename(\"temp_audio.mp3\", output_path)\n",
        "                return output_path\n",
        "            return None\n",
        "\n",
        "        except yt_dlp.DownloadError as e:\n",
        "            if \"Sign in to confirm youâ€™re not a bot\" in str(e):\n",
        "                 print(f\"\\nâŒ Download Error: Bot detection block. Please ensure the cookies file exists at '{cookies_path}' and is valid.\")\n",
        "            else:\n",
        "                print(f\"\\nâŒ Download Error: {e}\")\n",
        "            return None\n",
        "        except Exception as e:\n",
        "            print(f\"\\nâŒ An unexpected error occurred: {e}\")\n",
        "            return None\n",
        "\n",
        "    def transcribe_and_chunk(self, audio_path, max_duration=15.0):\n",
        "        \"\"\"\n",
        "        Transcribes, aligns, and performs the two-pass semantic chunking.\n",
        "        \"\"\"\n",
        "        print(\" Transcribing audio (with word timestamps)...\")\n",
        "        result = self.model.transcribe(audio_path, word_timestamps=True)\n",
        "\n",
        "        # --- 1. Alignment: Get Sentences with Timestamps ---\n",
        "        print(\"â±ï¸ Aligning sentences to timestamps...\")\n",
        "        all_words = []\n",
        "        for segment in result['segments']:\n",
        "            if 'words' in segment:\n",
        "                all_words.extend(segment['words'])\n",
        "            else:\n",
        "                 # Fallback for old Whisper versions or specific audio segments\n",
        "                 print(\"Warning: Missing word-level timestamps in a segment.\")\n",
        "\n",
        "        sentences = nltk.sent_tokenize(result['text'])\n",
        "        aligned_sentences = []\n",
        "        word_idx = 0\n",
        "\n",
        "        for sent in sentences:\n",
        "            sent_obj = {\"text\": sent, \"start\": None, \"end\": None, \"words\": []}\n",
        "            sent_clean = sent.replace(\" \", \"\").lower()\n",
        "            acc = \"\"\n",
        "\n",
        "            while word_idx < len(all_words):\n",
        "                w = all_words[word_idx]\n",
        "\n",
        "                # Check if the word belongs to the current sentence\n",
        "                if sent_obj[\"start\"] is None: sent_obj[\"start\"] = w[\"start\"]\n",
        "                sent_obj[\"end\"] = w[\"end\"]\n",
        "                sent_obj[\"words\"].append(w)\n",
        "\n",
        "                acc += w[\"word\"].replace(\" \", \"\").lower()\n",
        "                word_idx += 1\n",
        "\n",
        "                # Heuristic break: when accumulated word characters match sentence length (with tolerance)\n",
        "                if len(acc) >= len(sent_clean) * 0.9: break\n",
        "\n",
        "            aligned_sentences.append(sent_obj)\n",
        "\n",
        "        # --- 2. Chunking: Pass 1 (Aggregation) & Pass 2 (Enforcement) ---\n",
        "        print(\"ðŸ§  Performing semantic chunking...\")\n",
        "        chunks = []\n",
        "        current_chunk = {\"text\": \"\", \"start\": None, \"end\": None, \"sentences\": [], \"words\": []}\n",
        "\n",
        "        # PASS 1: AGGREGATION\n",
        "        for sent in aligned_sentences:\n",
        "            if sent[\"start\"] is None: continue\n",
        "\n",
        "            # If current chunk is empty, initialize it\n",
        "            if not current_chunk[\"sentences\"]:\n",
        "                current_chunk[\"start\"] = sent[\"start\"]\n",
        "                current_chunk[\"end\"] = sent[\"end\"]\n",
        "                current_chunk[\"text\"] = sent[\"text\"]\n",
        "                current_chunk[\"sentences\"] = [sent]\n",
        "                current_chunk[\"words\"].extend(sent[\"words\"])\n",
        "                continue\n",
        "\n",
        "            # Calculate theoretical new duration\n",
        "            new_end = sent[\"end\"]\n",
        "            current_duration = new_end - current_chunk[\"start\"]\n",
        "\n",
        "            if current_duration <= max_duration:\n",
        "                # Add to current chunk (Semantic Coherence)\n",
        "                current_chunk[\"end\"] = new_end\n",
        "                current_chunk[\"text\"] += \" \" + sent[\"text\"]\n",
        "                current_chunk[\"sentences\"].append(sent)\n",
        "                current_chunk[\"words\"].extend(sent[\"words\"])\n",
        "            else:\n",
        "                # Force Break (Time Constraint)\n",
        "                chunks.append(current_chunk)\n",
        "\n",
        "                # Start new chunk with the current sentence\n",
        "                current_chunk = {\n",
        "                    \"start\": sent[\"start\"],\n",
        "                    \"end\": sent[\"end\"],\n",
        "                    \"text\": sent[\"text\"],\n",
        "                    \"sentences\": [sent],\n",
        "                    \"words\": sent[\"words\"]\n",
        "                }\n",
        "\n",
        "        if current_chunk[\"start\"] is not None:\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "        # PASS 2: ENFORCEMENT (Splitting Oversized Chunks)\n",
        "        final_chunks = []\n",
        "        chunk_counter = 1\n",
        "\n",
        "        for chunk in chunks:\n",
        "            duration = chunk[\"end\"] - chunk[\"start\"]\n",
        "\n",
        "            if duration <= max_duration:\n",
        "                final_chunks.append({\n",
        "                    \"chunk_id\": chunk_counter,\n",
        "                    \"chunk_length\": round(duration, 2),\n",
        "                    \"text\": chunk[\"text\"].strip(),\n",
        "                    \"start_time\": round(chunk[\"start\"], 2),\n",
        "                    \"end_time\": round(chunk[\"end\"], 2)\n",
        "                })\n",
        "                chunk_counter += 1\n",
        "            else:\n",
        "                # Oversized: Must split using word boundaries/pauses\n",
        "                # Simple implementation: split at the word closest to max_duration\n",
        "                sub_chunks = self._split_oversized_by_duration(chunk, max_duration, chunk_counter)\n",
        "                final_chunks.extend(sub_chunks)\n",
        "                chunk_counter += len(sub_chunks)\n",
        "\n",
        "        return final_chunks\n",
        "\n",
        "    def _split_oversized_by_duration(self, chunk, max_duration, start_id):\n",
        "        \"\"\"Splits oversized chunks using word timestamps to respect max_duration.\"\"\"\n",
        "        sub_chunks = []\n",
        "        words = chunk[\"words\"]\n",
        "        current_start = words[0][\"start\"]\n",
        "        sub_chunk_words = []\n",
        "\n",
        "        for i, word in enumerate(words):\n",
        "            sub_chunk_words.append(word)\n",
        "            current_duration = word[\"end\"] - current_start\n",
        "\n",
        "            # Check if adding the NEXT word will exceed the limit\n",
        "            next_word_end = words[i+1][\"end\"] if i+1 < len(words) else word[\"end\"]\n",
        "            next_duration = next_word_end - current_start\n",
        "\n",
        "            # If next duration exceeds or this is the last word\n",
        "            if next_duration > max_duration or i == len(words) - 1:\n",
        "                # Finalize the current sub-chunk at the current word boundary\n",
        "\n",
        "                # Use the last word's end time and the first word's start time\n",
        "                end_time = sub_chunk_words[-1][\"end\"]\n",
        "\n",
        "                sub_chunks.append({\n",
        "                    \"chunk_id\": start_id + len(sub_chunks),\n",
        "                    \"chunk_length\": round(end_time - current_start, 2),\n",
        "                    \"text\": \" \".join([w[\"word\"] for w in sub_chunk_words]).strip(),\n",
        "                    \"start_time\": round(current_start, 2),\n",
        "                    \"end_time\": round(end_time, 2)\n",
        "                })\n",
        "\n",
        "                # Reset for the next sub-chunk\n",
        "                if i < len(words) - 1:\n",
        "                    current_start = words[i+1][\"start\"]\n",
        "                    sub_chunk_words = []\n",
        "\n",
        "        return sub_chunks\n",
        "\n",
        "\n",
        "# --- MAIN EXECUTION AND GRADIO APP ---\n",
        "\n",
        "def process_video(url, cookies_path=\"/content/cookies.txt\"):\n",
        "    \"\"\"\n",
        "    Main function to run the full pipeline.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(cookies_path):\n",
        "        return json.dumps({\"error\": f\"Cookies file not found at: {cookies_path}. Please create the file to bypass YouTube's sign-in requirement.\"}, indent=4)\n",
        "\n",
        "    try:\n",
        "        chunker = RobustVideoChunker(model_size=\"base\") # Change to 'large-v3' for max quality\n",
        "\n",
        "        # 1. Download (Cookies method)\n",
        "        audio_file = chunker.download_audio(url, cookies_path)\n",
        "\n",
        "        if not audio_file:\n",
        "            return json.dumps({\"error\": \"Download failed. Check console for details (e.g., cookie validity, file permissions).\"}, indent=4)\n",
        "\n",
        "        # 2. Transcribe and Chunk\n",
        "        chunks = chunker.transcribe_and_chunk(audio_file)\n",
        "\n",
        "        # Cleanup\n",
        "        if os.path.exists(audio_file):\n",
        "            os.remove(audio_file)\n",
        "\n",
        "        return json.dumps(chunks, indent=4)\n",
        "\n",
        "    except Exception as e:\n",
        "        return json.dumps({\"error\": f\"An unexpected processing error occurred: {str(e)}\"}, indent=4)\n",
        "\n",
        "# --- BONUS: Gradio App Interface ---\n",
        "def launch_app():\n",
        "    iface = gr.Interface(\n",
        "        fn=process_video,\n",
        "        inputs=[\n",
        "            gr.Textbox(label=\"YouTube URL\", placeholder=\"e.g., https://www.youtube.com/watch?v=jNQXAC9IVRw\"),\n",
        "            gr.Textbox(label=\"Cookies File Path\", value=\"/content/cookies.txt\", placeholder=\"e.g., my_cookies.txt\")\n",
        "        ],\n",
        "        outputs=gr.JSON(label=\"Semantic Chunks (List of Dictionaries)\"),\n",
        "        title=\"YouTube Semantic Chunker with Cookies Fix\",\n",
        "        description=\"Extracts high-precision, semantically rich audio-text segments (max 15s) from any YouTube video. Requires a cookies file to bypass sign-in.\"\n",
        "    )\n",
        "    iface.launch(share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Uncomment the line below to launch the Gradio web interface ---\n",
        "    launch_app()\n",
        "\n",
        "    # --- Or, run locally for testing (Make sure your youtube_cookies.txt is ready) ---\n",
        "    #test_url = \"https://www.youtube.com/watch?v=jNQXAC9IVRw\"\n",
        "    #print(process_video(test_url))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_url = \"https://www.youtube.com/watch?v=1aA1WGON49E\"\n",
        "print(process_video(test_url))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiGzidvS54Ln",
        "outputId": "a4c36c0c-9eac-4c69-d645-b88545e56274"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Whisper model 'base' on cuda...\n",
            "Model loaded.\n",
            "â¬‡ï¸ Downloading audio from https://www.youtube.com/watch?v=1aA1WGON49E using cookies from: /content/cookies.txt\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=1aA1WGON49E\n",
            "[youtube] 1aA1WGON49E: Downloading webpage\n",
            "[youtube] 1aA1WGON49E: Downloading tv downgraded player API JSON\n",
            "[youtube] 1aA1WGON49E: Downloading web safari player API JSON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING: [youtube] 1aA1WGON49E: n challenge solving failed: Some formats may be missing. Ensure you have a supported JavaScript runtime and challenge solver script distribution installed. Review any warnings presented before this message. For more details, refer to  https://github.com/yt-dlp/yt-dlp/wiki/EJS\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] 1aA1WGON49E: Downloading m3u8 information\n",
            "[info] 1aA1WGON49E: Downloading 1 format(s): 96\n",
            "[download] Sleeping 6.00 seconds as required by the site...\n",
            "[hlsnative] Downloading m3u8 manifest\n",
            "[hlsnative] Total fragments: 16\n",
            "[download] Destination: temp_audio\n",
            "[download] 100% of  155.91MiB in 00:00:00 at 261.95GiB/s\n",
            "[FixupM3u8] Fixing MPEG-TS in MP4 container of \"temp_audio\"\n",
            "[ExtractAudio] Destination: temp_audio.mp3\n",
            "Deleting original file temp_audio (pass -k to keep)\n",
            " Transcribing audio (with word timestamps)...\n",
            "â±ï¸ Aligning sentences to timestamps...\n",
            "ðŸ§  Performing semantic chunking...\n",
            "[\n",
            "    {\n",
            "        \"chunk_id\": 1,\n",
            "        \"chunk_length\": 5.64,\n",
            "        \"text\": \"Welcome to this course on mastering technical interviews for software engineering roles.\",\n",
            "        \"start_time\": 0.0,\n",
            "        \"end_time\": 5.64\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 2,\n",
            "        \"chunk_length\": 11.02,\n",
            "        \"text\": \"These interviews often hinge on your ability to confidently solve problems using data structures and algorithms, which can feel challenging without the right foundation.\",\n",
            "        \"start_time\": 5.64,\n",
            "        \"end_time\": 16.66\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 3,\n",
            "        \"chunk_length\": 14.6,\n",
            "        \"text\": \"the  right  foundation.  In  this  conference,  of  course,  path  from  destination  fang  will  strip  away  the  intimidation  and  equip  you  with  the  essential  knowledge  starting  with  the  core  concepts  of  data  structure,  algorithm,  and\",\n",
            "        \"start_time\": 16.66,\n",
            "        \"end_time\": 31.26\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 4,\n",
            "        \"chunk_length\": 0.44,\n",
            "        \"text\": \"fundamental\",\n",
            "        \"start_time\": 31.26,\n",
            "        \"end_time\": 31.7\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 5,\n",
            "        \"chunk_length\": 7.68,\n",
            "        \"text\": \"You'll learn how to judge an algorithm's efficiency using big O notation and apply this important skill to code blocks.\",\n",
            "        \"start_time\": 31.7,\n",
            "        \"end_time\": 39.38\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 6,\n",
            "        \"chunk_length\": 14.22,\n",
            "        \"text\": \"The course covers almost every major data structure and algorithm pattern from arrays, linked lists, stacks, and cues to advanced topics like trees, graphs, dynamic programming, and backtracking. So good luck learning.\",\n",
            "        \"start_time\": 39.38,\n",
            "        \"end_time\": 53.6\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 7,\n",
            "        \"chunk_length\": 11.16,\n",
            "        \"text\": \"Hello friends, hope you're having a fantastic day today. This is going to be 50 hour long course that contains everything there is to know about technical interviews, data structures, and algorithms.\",\n",
            "        \"start_time\": 53.6,\n",
            "        \"end_time\": 64.76\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 8,\n",
            "        \"chunk_length\": 13.16,\n",
            "        \"text\": \"There are about 20 concepts that I have created that comprise the entirety of technical interviews and that contains different data structures, different algorithms, and different coding patterns. We are going to cover each and everyone in very much detail.\",\n",
            "        \"start_time\": 64.76,\n",
            "        \"end_time\": 77.92\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 9,\n",
            "        \"chunk_length\": 6.12,\n",
            "        \"text\": \"We will understand the concept for them and then we would solve lots and lots of interview problems associated with that.\",\n",
            "        \"start_time\": 77.92,\n",
            "        \"end_time\": 84.04\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 10,\n",
            "        \"chunk_length\": 14.76,\n",
            "        \"text\": \"By the end of this course, you would have solved close to 200 most tasks, most like and most popular interview problems, and you would be able to solve majority of technical interview problems out there and then it's just a matter of sheer practice that you will have to do on your part.\",\n",
            "        \"start_time\": 84.04,\n",
            "        \"end_time\": 98.8\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 11,\n",
            "        \"chunk_length\": 11.22,\n",
            "        \"text\": \"I can guarantee you that this is going to be a very high quality in depth course where it is completely beginner friendly and we will go deeper and deeper into the most advanced topics and I will try to explain it in as much detail as I can.\",\n",
            "        \"start_time\": 98.8,\n",
            "        \"end_time\": 110.02\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 12,\n",
            "        \"chunk_length\": 12.46,\n",
            "        \"text\": \"So you can be absolutely prepared for anything to come in your technical interviews. Now a quick introduction about myself. My name is Parthware's and I have bachelors and masters in computer science.\",\n",
            "        \"start_time\": 110.02,\n",
            "        \"end_time\": 122.48\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 13,\n",
            "        \"chunk_length\": 10.68,\n",
            "        \"text\": \"I have been in the IT industry for close to 10 years now and I have worked at companies like Microsoft RBC Nokia and currently working as a solutions architect for synoplex. I have joined my career as a junior developer and have gone through multiple transitions.\",\n",
            "        \"start_time\": 122.48,\n",
            "        \"end_time\": 133.16\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 14,\n",
            "        \"chunk_length\": 8.16,\n",
            "        \"text\": \"I have been absolutely blessed by a lot of important mentors who have guided me correctly in my career and this is something me giving back to the community.\",\n",
            "        \"start_time\": 133.16,\n",
            "        \"end_time\": 141.32\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 15,\n",
            "        \"chunk_length\": 7.0,\n",
            "        \"text\": \"Now I also run my own YouTube channel named Destination Fang and I would highly encourage you if you can go and support me on that channel as well.\",\n",
            "        \"start_time\": 141.32,\n",
            "        \"end_time\": 148.32\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 16,\n",
            "        \"chunk_length\": 12.42,\n",
            "        \"text\": \"I teach about how you can go ahead in your career and how you can ace your technical interviews, behavioral interviews and system design interviews and your support matters a lot for me. So it can motivate me to keep making more videos like this.\",\n",
            "        \"start_time\": 148.32,\n",
            "        \"end_time\": 160.74\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 17,\n",
            "        \"chunk_length\": 15.0,\n",
            "        \"text\": \"On top of it you can stay up to date on the latest and greatest things that I am currently working on. Few fun facts about me. I love going on hikes, I love watching movies and I love playing with my twin daughters. I am based in Canada and this is a beautiful country. So if you get a chance to visit please come and see it anytime.\",\n",
            "        \"start_time\": 160.74,\n",
            "        \"end_time\": 175.74\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 18,\n",
            "        \"chunk_length\": 12.28,\n",
            "        \"text\": \"Now I would not deviate from the main topic that why we are here. So without any delay let us get started. Now what is DSA? DSA stands for data structure and algorithm. Now everything in computers is made out of data.\",\n",
            "        \"start_time\": 175.74,\n",
            "        \"end_time\": 188.02\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 19,\n",
            "        \"chunk_length\": 12.88,\n",
            "        \"text\": \"These are the bits and pieces of zeros and ones that hold all the information. It can be a text or a book or a PDF or the video you are watching. Anything can be represented as data and the way we store and organize the data is referred to as data structures.\",\n",
            "        \"start_time\": 188.02,\n",
            "        \"end_time\": 200.9\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 20,\n",
            "        \"chunk_length\": 7.18,\n",
            "        \"text\": \"Now there are many different ways of storing data. You can store them in a straight fixed given format that can be defined as area.\",\n",
            "        \"start_time\": 201.2,\n",
            "        \"end_time\": 208.38\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 21,\n",
            "        \"chunk_length\": 14.44,\n",
            "        \"text\": \"You can have one bit of data live over here, next live over here and after one the next lives over here but this guide does not know where this guy exists and that is an example of linked disk and there are bunch of other ways to understanding different data structures.\",\n",
            "        \"start_time\": 208.38,\n",
            "        \"end_time\": 222.82\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 22,\n",
            "        \"chunk_length\": 14.08,\n",
            "        \"text\": \"Now there are linear data structures, non-linear data structures, hierarchical data structures, hash based data structures and they all have different purposes which we will learn more in the classification of data structures. Now let us understand that what is algorithm?\",\n",
            "        \"start_time\": 222.82,\n",
            "        \"end_time\": 236.9\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 23,\n",
            "        \"chunk_length\": 6.0,\n",
            "        \"text\": \"Algorithm is nothing but following step by step to complete certain procedure and achieve an expected outcome. Now this can be anything.\",\n",
            "        \"start_time\": 236.9,\n",
            "        \"end_time\": 242.9\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 24,\n",
            "        \"chunk_length\": 14.68,\n",
            "        \"text\": \"data  structures  and  they  all  have  different  purposes  which  we  will  learn  more  in  the  classification  of  data  structures.  Now  let  us  understand  that  what  is  algorithm?  Algorithm  is  nothing  but  following  step  by  step  to  complete  certain  procedure  and  achieve\",\n",
            "        \"start_time\": 242.9,\n",
            "        \"end_time\": 257.58\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 25,\n",
            "        \"chunk_length\": 5.8,\n",
            "        \"text\": \"an  expected  outcome.  Now  this  can  be  anything.  If  we  strictly  talk  about  from  the\",\n",
            "        \"start_time\": 257.58,\n",
            "        \"end_time\": 263.38\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 26,\n",
            "        \"chunk_length\": 7.72,\n",
            "        \"text\": \"We just do not realize that these are algorithms and then there is the combination of data section algorithms that help us solve many complicated problems.\",\n",
            "        \"start_time\": 263.38,\n",
            "        \"end_time\": 271.1\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 27,\n",
            "        \"chunk_length\": 14.78,\n",
            "        \"text\": \"in  the  existing  given  data  structure,  recursion,  dynamic  programming,  greedy  approach,  divide  and  conquer  and  these  are  all  the  strategies  that  we  typically  use  in  our  day  to  day  operations  as  well.  We  just  do  not  realize  that  these  are  algorithms  and\",\n",
            "        \"start_time\": 271.1,\n",
            "        \"end_time\": 285.88\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 28,\n",
            "        \"chunk_length\": 11.96,\n",
            "        \"text\": \"then  there  is  the  combination  of  data  section  algorithms  that  help  us  solve  many  complicated  problems.  So  that  is  why  the  companies  that  are  substantially  large  in  the  IT  industry,\",\n",
            "        \"start_time\": 285.88,\n",
            "        \"end_time\": 297.84\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 29,\n",
            "        \"chunk_length\": 12.22,\n",
            "        \"text\": \"Now how does a solution can be defined as optimal? Well there are two things that defines that and that is called space complexity and time complexity and they are usually referred by big old notation.\",\n",
            "        \"start_time\": 298.02,\n",
            "        \"end_time\": 310.24\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 30,\n",
            "        \"chunk_length\": 8.08,\n",
            "        \"text\": \"Now we will understand the full theory on that in one of the sections but how does companies typically evaluate your solution?\",\n",
            "        \"start_time\": 310.24,\n",
            "        \"end_time\": 318.32\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 31,\n",
            "        \"chunk_length\": 9.12,\n",
            "        \"text\": \"They are looking for number one whether you can reach to the optimal solution or not, that is a basic criteria but that is not just the only criteria.\",\n",
            "        \"start_time\": 318.32,\n",
            "        \"end_time\": 327.44\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 32,\n",
            "        \"chunk_length\": 14.78,\n",
            "        \"text\": \"are  two  things  that  defines  that  and  that  is  called  space  complexity  and  time  complexity  and  they  are  usually  referred  by  big  old  notation.  Now  we  will  understand  the  full  theory  on  that  in  one  of  the  sections  but  how  does  companies  typically  evaluate  your\",\n",
            "        \"start_time\": 327.44,\n",
            "        \"end_time\": 342.22\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 33,\n",
            "        \"chunk_length\": 0.42,\n",
            "        \"text\": \"solution?\",\n",
            "        \"start_time\": 342.22,\n",
            "        \"end_time\": 342.64\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 34,\n",
            "        \"chunk_length\": 13.04,\n",
            "        \"text\": \"Now you are being judged based on your thinking abilities, your communication whether you are a good person to work with can you think of different scenarios, different edge cases are you able to come up with the optimal solution? What does your approach looks like?\",\n",
            "        \"start_time\": 343.04,\n",
            "        \"end_time\": 356.08\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 35,\n",
            "        \"chunk_length\": 12.52,\n",
            "        \"text\": \"If you have a suboptimal solution can you reach to that optimal solution? Can you ask for proper hints? Can you convince your recruiter or interviewer about your solution? Can you walk through it? Can you code it out?\",\n",
            "        \"start_time\": 356.08,\n",
            "        \"end_time\": 368.6\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 36,\n",
            "        \"chunk_length\": 11.46,\n",
            "        \"text\": \"And you know the complicated part is they typically do not give you a full IDE or editor. They typically ask you to solve this problem on a whiteboard if it is in person or on a Google Doc if that is going to be a virtual interview.\",\n",
            "        \"start_time\": 368.6,\n",
            "        \"end_time\": 380.06\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 37,\n",
            "        \"chunk_length\": 14.06,\n",
            "        \"text\": \"So that is why there are a combination of skills that you will have to accumulate throughout this course and your preparation journey to be able to clear multiple different technical interviews and we will learn that in the next section. Now how do we even judge an algorithm?\",\n",
            "        \"start_time\": 380.06,\n",
            "        \"end_time\": 394.12\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 38,\n",
            "        \"chunk_length\": 2.24,\n",
            "        \"text\": \"Well there are three things that we are looking for.\",\n",
            "        \"start_time\": 394.12,\n",
            "        \"end_time\": 396.36\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 39,\n",
            "        \"chunk_length\": 14.76,\n",
            "        \"text\": \"do  not  give  you  a  full  IDE  or  editor.  They  typically  ask  you  to  solve  this  problem  on  a  whiteboard  if  it  is  in  person  or  on  a  Google  Doc  if  that  is  going  to  be  a  virtual  interview.  So  that  is  why  there  are  a  combination  of\",\n",
            "        \"start_time\": 396.36,\n",
            "        \"end_time\": 411.12\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 40,\n",
            "        \"chunk_length\": 4.52,\n",
            "        \"text\": \"skills  that  you  will  have  to  accumulate  throughout  this  course  and  your  preparation\",\n",
            "        \"start_time\": 411.12,\n",
            "        \"end_time\": 415.64\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 41,\n",
            "        \"chunk_length\": 9.8,\n",
            "        \"text\": \"Second thing is time complexity and third thing is space complexity. So first let's understand that what is time complexity through an example by these deck of cards.\",\n",
            "        \"start_time\": 415.64,\n",
            "        \"end_time\": 425.44\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 42,\n",
            "        \"chunk_length\": 14.92,\n",
            "        \"text\": \"Time complexity is nothing but how much longer will it take for you to complete your algorithm or your task if the number of inputs keeps on growing larger and larger and larger. Let's try to understand that currently I have these deck of cards and they are currently just randomized.\",\n",
            "        \"start_time\": 425.44,\n",
            "        \"end_time\": 440.36\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 43,\n",
            "        \"chunk_length\": 6.06,\n",
            "        \"text\": \"If I lay them flat on a table and let's say that for simplicity I am trying to figure out where does heart of six lies amongst these cards.\",\n",
            "        \"start_time\": 440.36,\n",
            "        \"end_time\": 446.42\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 44,\n",
            "        \"chunk_length\": 13.96,\n",
            "        \"text\": \"Now I don't know but I can keep on opening all the different cards randomly until eventually I found out to a card that is heart of six and notice that I found heart of six right here. But how many cards did it took me to find this value?\",\n",
            "        \"start_time\": 446.42,\n",
            "        \"end_time\": 460.38\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 45,\n",
            "        \"chunk_length\": 13.32,\n",
            "        \"text\": \"It's just like finding a needle in a haystack you need to go one by one and if I had like million cards and all different unique entries it might take me like million attempts before I reach to the card that I am particularly looking for.\",\n",
            "        \"start_time\": 460.38,\n",
            "        \"end_time\": 473.7\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 46,\n",
            "        \"chunk_length\": 9.3,\n",
            "        \"text\": \"So in this case we can notice that the time it takes for me to complete the expected task increases linearly with the increase in the number of inputs.\",\n",
            "        \"start_time\": 473.7,\n",
            "        \"end_time\": 483.0\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 47,\n",
            "        \"chunk_length\": 14.94,\n",
            "        \"text\": \"So we can call this time complexity of big O of n. Now big O is the definition or a way we write the time complexity in space complexity and we are going to talk all about that in a separate topic. But now let's just say focus towards this.\",\n",
            "        \"start_time\": 483.0,\n",
            "        \"end_time\": 497.94\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 48,\n",
            "        \"chunk_length\": 8.14,\n",
            "        \"text\": \"Now let's see that how does time complexity change based off of how we have our data being scattered or how we change our algorithm. Let's take an example.\",\n",
            "        \"start_time\": 497.94,\n",
            "        \"end_time\": 506.08\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 49,\n",
            "        \"chunk_length\": 14.52,\n",
            "        \"text\": \"Now in this case we are going to sort all of these cards in ascending order and then we are going to lay them flat once again on our card. So this is going to be the smallest value and then we are going to keep on adding all of these values on this table.\",\n",
            "        \"start_time\": 506.08,\n",
            "        \"end_time\": 520.6\n",
            "    },\n",
            "    {\n",
            "        \"chunk_id\": 50,\n",
            "        \"chunk_length\": 1.06,\n",
            "        \"text\": \"Now note.\",\n",
            "        \"start_time\": 520.6,\n",
            "        \"end_time\": 521.66\n",
            "    }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "launch_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "ZsGPgfFdxf0k",
        "outputId": "c9d93b09-14d9-4bd8-f424-d567c907f890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d95ea63ad24849b7b5.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d95ea63ad24849b7b5.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}